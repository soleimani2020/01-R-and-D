# ğŸ“ˆ Linear Regression Using Gradient Descent 

**Difficulty:** Easy  
**Category:** Machine Learning  

This project implements Linear Regression from scratch using Batch Gradient Descent in Python with NumPy.

The goal is to minimize the Mean Squared Error (MSE) loss function and learn the optimal model coefficients (weights).

---

## ğŸ“Œ Problem Description

Write a Python function that performs linear regression using gradient descent.

The function must:

- Accept:
  - `X`: Feature matrix including a column of ones for the intercept  
  - `y`: Target vector  
  - `alpha`: Learning rate  
  - `iterations`: Number of gradient descent steps  

- Return:
  - The learned coefficient vector `theta` as a NumPy array  

---

## ğŸ“ Mathematical Formulation

We minimize the Mean Squared Error (MSE) loss:

L(Î¸) = (1 / 2m) * Î£ (hÎ¸(xá¶¦) âˆ’ yá¶¦)Â²

Where:

- m = number of training samples  
- Î¸ = parameter vector  
- hÎ¸(x) = XÎ¸ = prediction function  

The factor 1/2 simplifies the gradient expression.

---

## ğŸ”„ Gradient Descent Update Rule

Gradient of the loss function:

âˆ‡L(Î¸) = (1/m) Xáµ€ (XÎ¸ âˆ’ y)

Update step:

Î¸ := Î¸ âˆ’ Î± âˆ‡L(Î¸)

Where:

- Î± = learning rate  

---

## ğŸ“Š Input Specifications

| Variable | Shape | Description |
|----------|--------|------------|
| `X` | `(m, n)` | Feature matrix (includes bias column of ones) |
| `y` | `(m,)` | Target values |
| `theta` | `(n,)` | Model weights |

- m = number of training examples  
- n = number of features (including bias)

---



Video Tutorial: https://www.youtube.com/watch?v=U6Z-UjkJZjA&t=470s

Video Tutorial: https://www.youtube.com/@NicholasRenotte/featured 





